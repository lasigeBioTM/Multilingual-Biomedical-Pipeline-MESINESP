# Multilingual Biomedical Pipeline MESINESP
 Developed solution by the LASIGE BioTM team for the first edition of BioASQ MESINESP task. It consists in the combination of X-BERT/X-Transformer, an Extreme Multi-Label Classification algorithm, with MER, a named entity recognition software.
  
 You can know more about X-Transformer in: https://github.com/OctoberChang/X-Transformer
 
 
 ## Requirements:
 The code requires the following: 
 
- merpy

- gawk

- pandas

- Transformers 2.2.2

- nltk

- scipy

- sklearn

- h5py

- allennlp

- pytorch_pretrained_bert



 ## HOW TO RUN / TRAIN A MODEL:
 ### 1) Download MESINESP data
In the MESINESP/mesinesp_data folder, run the 'get_data_mesinesp.sh' script to download the data.


### 2) Generate MER Lexicons
Go to the mer_scripts folder and execute the 'gen_mer_lexs.sh' script. This will generate the MER lexicon using the DeCS data.


### 3) Process data
In the MESINESP folder, execute the 'proc_data_mesinesp.sh' script. It is required to choose which version of X-Transformer to use, i.e, if you want to use the older version (X-BERT) or the recent version (X-Transformer). It also requires a name to give to the dataset. 

Example: 
./proc_data_mesinesp.sh X-Transformer mesinesp


NOTE: the script was made considering a default number of CPUs = 15. If you have more or less CPUs, it is recommended to change their number in the script.


### 4) Run X-BERT/X-Transformer
According to the chosen algorithm in the previous step, go to the corresponding folder (X-BERT or X-Transformer) and execute the script. You need to specify the name of the dataset, which was defined in the previous step, and the pre-trained model to use.  

Example for X-BERT: 
./run_xbert.sh mesinesp bert-base-multilingual-cased

Example for X-Transformer: 
./run_X-Transformer_mesinesp.sh mesinesp bert-base-multilingual-cased


NOTE: Both scripts consider a default number of CPUs = 15. Also, the X-Transformer script considers 4 GPUs while the X-BERT script considers only 1. If you have a different number of CPU/GPUs, it is recommended that you change these numbers, otherwise the scripts might fail.

NOTE: It might take some days to train the model


### 5) Process .npz file and generate output
Return to the MESINESP folder and execute the python script 'proc_npz.py'. This will process the .npz file generated by X-BERT/X-Transformer and then generate 4 .json files with the predictions for each article. Each file has the predicted labels according to a certain threshold value that discards each label with a confidence value under that threshold. This way, the four files have different focus, namely the highest Precision, Recall, F1-Score and one using the baseline score, which corresponds to the threshold score = 0.

The script has the following parameters:

-npz: the path to the .npz file.

-i: path to the folder that contains the label_vocab.txt and the test.txt from which the predictions were made. 

-o: path to store the .json files. 
